<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Mathematical Formulation • NPVecchia</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Mathematical Formulation">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">NPVecchia</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/math.html">Mathematical Formulation</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Mathematical Formulation</h1>
            
      
      
      <div class="hidden name"><code>math.Rmd</code></div>

    </div>

    
    

<p>All of these will be introduced in the text, but for a quick reference they are listed here.</p>
<p><span class="math inline">\(n\)</span>: the number of (spatial) locations with observations</p>
<p><span class="math inline">\(N\)</span> : the number of observations per location</p>
<p><span class="math inline">\(m\)</span> : the number of neighbors for calculation</p>
<p><span class="math inline">\(\alpha_i\)</span> : the shape parameter for the IG prior in the i’th regression</p>
<p><span class="math inline">\(\beta_i\)</span> : the scale parameter for the IG prior in the i’th regression</p>
<p><span class="math inline">\(a_i, b_i\)</span> : posterior IG parameters</p>
<p><span class="math inline">\({\bm{\Gamma}}_i\)</span> : the prior variance on the coefficients in the i’th regression</p>
<p><span class="math inline">\({\mathbf{G}}_i\)</span> : posterior variance</p>
<p><span class="math inline">\(\hat{{\mathbf{U}}} = {\mathbf{U}}{\mathbf{D}}^{-1/2}\)</span> : Cholesky of the precision matrix</p>

Assume we have <span class="math inline">\(N \geq 1\)</span> observations of a continuous spatial process at <span class="math inline">\(n\)</span> locations (in low dimensional space). We model the detrended (i.e., centered) data as
<span class="math display">\[\begin{equation}
\label{eq:datamodel}
\bz^{(\ell)} | \bfSigma \overset{iid}{\sim} \normal_n(\bfzero,\bfSigma), \qquad \ell = 1,\ldots,N,
\end{equation}\]</span>
<p>where <span class="math inline">\({\mathbf{z}}^{(\ell)} = (z_1^{(\ell)},\ldots,z_n^{(\ell)})'\)</span>, and <span class="math inline">\(z_i^{(\ell)}\)</span> is observed at spatial location <span class="math inline">\({\mathbf{s}}_i\)</span>. We denote by <span class="math inline">\({\mathbf{z}}\)</span> all observations <span class="math inline">\({\mathbf{z}}^{(1)},\ldots,{\mathbf{z}}^{(N)}\)</span> stacked into a long vector. We assume that the locations <span class="math inline">\({\mathbf{s}}_1,\ldots,{\mathbf{s}}_n\)</span>, and hence the corresponding variables <span class="math inline">\(z_i^{(\ell)}\)</span> in <span class="math inline">\({\mathbf{z}}^{(\ell)}\)</span>, are ordered according to a maximin ordering .</p>
<p>Our goal is to make inference on the spatial covariance matrix <span class="math inline">\({\bm{\Sigma}}\)</span> based on the data <span class="math inline">\({\mathbf{z}}\)</span>, in the case where <span class="math inline">\(n\)</span> is large (in the hundreds or even hundreds of thousands) and <span class="math inline">\(N\)</span> is relatively small. Typically, a parametric, and often isotropic, covariance function is assumed to determine <span class="math inline">\({\bm{\Sigma}}\)</span> such that it only is a function of a very small number of parameters, which can then be estimated relatively easily. Here, we avoid explicit assumptions of stationarity and isotropy.</p>
Instead, we assume that a spatial screening effect holds, such that
<span class="math display">\[\begin{equation}
\label{eq:screening}
p(z_i^{(\ell)}|\bz_{1:i-1}^{(\ell)},\bfSigma) = p(z_i^{(\ell)}|\bz_{g_m(i)}^{(\ell)},\bfSigma),
\end{equation}\]</span>
<p>where <span class="math inline">\(g_m(i) \subset (1,\ldots,i-1)\)</span> is an index vector consisting of the indices of the <span class="math inline">\(\min(m,i-1)\)</span> nearest neighbors to <span class="math inline">\({\mathbf{s}}_i\)</span> among those ordered previously; that is, <span class="math inline">\({\mathbf{s}}_{(g_m(i))_j}\)</span> is the <span class="math inline">\(j\)</span>th nearest neighbor of <span class="math inline">\({\mathbf{s}}_i\)</span>. The equation  always holds trivially holds for <span class="math inline">\(m = n-1\)</span>, but for many covariances, it even holds (at least approximately) for <span class="math inline">\(m \ll n\)</span> due to the so-called screening effect. Assume for now that <span class="math inline">\(m\)</span> is fixed and known.</p>
<p>Consider the modified Cholesky decomposition of the inverse of <span class="math inline">\({\bm{\Sigma}}\)</span> (i.e., the precision matrix):</p>
<span class="math display">\[\begin{equation}
\label{eq:cholesky}
\bfSigma^{-1} = \bU \bD^{-1}\bU',
\end{equation}\]</span>
<p>where <span class="math inline">\({\mathbf{D}}= \text{diag}(d_1,\ldots,d_n)\)</span> is a diagonal matrix with positive entries <span class="math inline">\(d_i &gt; 0\)</span>, and <span class="math inline">\({\mathbf{U}}\)</span> is an upper triangular matrix with unit diagonal (i.e., <span class="math inline">\({\mathbf{U}}_{ii}=1\)</span>). The screening effect in  implies that <span class="math inline">\({\mathbf{U}}\)</span> is sparse, with at most <span class="math inline">\(m\)</span> nonzero off-diagonal elements per column . We define <span class="math inline">\({\mathbf{u}}_i = {\mathbf{U}}_{g_i,i}\)</span> as the nonzero off-diagonal entries in the <span class="math inline">\(i\)</span>th column.</p>

From , we see that we can estimate <span class="math inline">\({\bm{\Sigma}}\)</span> by inferring <span class="math inline">\(d_1,\ldots,d_n\)</span> and <span class="math inline">\({\mathbf{u}}_1,\ldots,{\mathbf{u}}_n\)</span>. To do so, note that our data model  can be written as a series of linear regression models :
<span class="math display">\[\begin{equation}
\label{eq:regression}
p(\bz | \bfSigma) = \prod_{i=1}^n p(\by_i|\by_{1:i-1} , \bfSigma) = \prod_{i=1}^n \normal_N(\by_i|\bX_i\bu_i,d_i\bI_N),
\end{equation}\]</span>
<p>where <span class="math inline">\({\mathbf{y}}_i = (z_i^{(1)},\ldots,z_i^{(N)})'\)</span>, and <span class="math inline">\({\mathbf{X}}_i\)</span> is an <span class="math inline">\(N \times m\)</span> matrix with <span class="math inline">\(\ell\)</span>th row <span class="math inline">\(-{\mathbf{z}}_{g_i}^{(\ell)}{}'\)</span>. Note the negative sign for the entries of <span class="math inline">\({\mathbf{X}}_i\)</span>. Further details on why this and  are pushed to Section 2.</p>
<p>For the regression models in , we assume the standard, conjugate priors to form a series of Bayesian regression models: <span class="math display">\[
 {\mathbf{u}}_i | d_i,{\bm{\theta}}\stackrel{ind.}{\sim} {\mathcal{N}}({\mathbf{0}},d_i{\bm{\Gamma}}_i), \qquad d_i | {\bm{\theta}}\stackrel{ind.}{\sim} \mathcal{IG}(\alpha_i,\beta_i),
\]</span> where <span class="math inline">\({\bm{\theta}}\)</span> is a vector of hyperparameters determining <span class="math inline">\(m\)</span>, <span class="math inline">\({\bm{\Gamma}}_i\)</span>, <span class="math inline">\(\alpha_i\)</span>, and <span class="math inline">\(\beta_i\)</span>, which will be discussed further below.</p>
Due to conjugacy, the posterior distribution (conditional on <span class="math inline">\({\bm{\theta}}\)</span>) is available in closed form:
<span class="math display">\[\begin{align}
p(\bu_1,\ldots,\bu_n,d_1,\ldots,d_n|\bz,\bftheta) &amp; = \prod_{i=1}^n p(\bu_i,d_i|\bz,\bftheta) = \prod_{i=1}^n p(\bu_i|d_i,\bz,\bftheta) \, p(d_i|\bz,\bftheta)\\
&amp; = \prod_{i=1}^n \normal(\bu_i|\hat\bu_i,d_i\bG_i) \, \mathcal{IG}(d_i|a_i,b_i), \label{eq:udpost}
\end{align}\]</span>
<p>where <span class="math inline">\(\hat{\mathbf{u}}_i = {\mathbf{G}}_i {\mathbf{X}}_i'{\mathbf{y}}_i\)</span>, <span class="math inline">\({\mathbf{G}}_i = ({\mathbf{X}}_i'{\mathbf{X}}_i + {\bm{\Gamma}}_i^{-1})^{-1}\)</span>, <span class="math inline">\(a_i = \alpha_i + N/2\)</span>, and <span class="math inline">\(b_i = \beta_i + ({\mathbf{y}}_i'({\mathbf{I}}_N+{\mathbf{X}}_i {\bm{\Gamma}}_i {\mathbf{X}}_i')^{-1}{\mathbf{y}}_i)/2 = \beta_i + ({\mathbf{y}}_i'{\mathbf{y}}_i - \hat{\mathbf{u}}_i' {\mathbf{G}}_i^{-1} \hat{\mathbf{u}}_i')/2\)</span>.</p>
<!-- Covariances and other posterior summaries of interest -->
<!-- Using \eqref{eq:udpost}, we can easily obtain samples or posterior summaries of the entries of $\bU$ and $\bD$ conditional on $\bftheta$. However, in many applications, primary interest will be in computing posterior summaries of $\bfSigma$ and other quantities. If $n$ is not too large ($n < 10^4$, say), we can simply compute $\bfSigma^{-1}$ and hence $\bfSigma$ from $\bU$ and $\bD$. For large $n$, it is usually not possible to even hold the entire dense matrix $\bfSigma$ in memory, but we can quickly compute useful summaries of it based on $\bU$ and $\bD$.  -->
<!-- In many applications, including climate-model emulation, it is of interest to sample new spatial fields from the model. We can sample $\bz^\star \sim \normal(\bfzero,\bfSigma)$ by sampling $\bw \sim \normal(\bfzero,\bI_n)$, and then setting $\bz^\star = (\bU \bD^{-1/2})^{-1} \bw $. If $\bU$ and $\bD$ are sampled from their posterior distribution given $\bz$, then we have obtained a sample from the posterior predictive distribution $p(\bz^\star|\bz)$. -->

<p>Previously, we have assumed the hyperparameters <span class="math inline">\({\bm{\theta}}\)</span> determining <span class="math inline">\(m\)</span>, <span class="math inline">\({\bm{\Gamma}}_i\)</span>, <span class="math inline">\(\alpha_i\)</span>, and <span class="math inline">\(\beta_i\)</span> to be fixed. We now discuss the inference of these hyperparameters.</p>
<p>First, assuming a hyperprior <span class="math inline">\(p({\bm{\theta}})\)</span> has been specified, the goal is to obtain the posterior distribution <span class="math inline">\(p({\bm{\theta}}|{\mathbf{z}}) \propto p({\mathbf{z}}|{\bm{\theta}})p({\bm{\theta}})\)</span>. While this distribution cannot be obtained analytically, we can sample from the posterior using the Metropolis-Hastings algorithm using the closed form of the marginal or integrated likelihood, <span class="math display">\[
p({\mathbf{z}}|{\bm{\theta}}) \propto \prod_{i=1}^n \sqrt{|{\mathbf{G}}_i|/|{\bm{\Gamma}}_i|} \times \beta_i^{\alpha_i}/b_i^{a_i} \times \Gamma(a_i)/\Gamma(\alpha_i),
\]</span> where the (non-bold) <span class="math inline">\(\Gamma\)</span> denotes the gamma function. Given the posterior distributions of <span class="math inline">\({\mathbf{U}},{\mathbf{D}}\)</span>, these evaluations are cheap computationally. Another alternative is to optimize these hyperparameters with this likelihood.</p>
<p>We now parameterize the prior distributions from before in terms of <span class="math inline">\({\bm{\theta}}= (\theta_1,\theta_2,\theta_3)'\)</span>, such that the resulting model shrinks toward an isotropic Mat'ern-type covariance. The parameter <span class="math inline">\(\theta_1\)</span> will play the role of a marginal variance, while <span class="math inline">\(\theta_2\)</span> and <span class="math inline">\(\theta_3\)</span> are related to the range and smoothness. For the package, we concatenate the prior parameters <span class="math inline">\(\alpha_i, \beta_i\)</span> into n-dimensional vectors <span class="math inline">\(a, b\)</span>, and the prior variance parameter <span class="math inline">\({\bm{\Gamma}}_i\)</span> (which is diagonal) into a matrix <span class="math inline">\({\mathbf{G}}\)</span> of dimension n by m as follows:</p>
<p><span class="math display">\[
\begin{aligned}
a &amp;= 6 \\
b &amp;= 5 e^{\theta_1} \bigg[ 1 - \text{exp} \bigg(- \frac{e^{\theta_2}}{\sqrt{0:(n-1)}} \bigg)  \bigg] \\
\text{temp} &amp;= \text{exp}\big(-e^{\theta_3} * (1:m) \big) \\
\text{each row of } {\mathbf{G}}&amp;= \frac{\text{temp}}{b_i / (a_i - 1)} \\
\end{aligned}
\]</span></p>
<p>For the method, we also provide a guideline for choosing <span class="math inline">\(m\)</span>. Our solution is to tie <span class="math inline">\(m\)</span> to the decay of the elements of <span class="math inline">\({\mathbf{U}}\)</span>. To allow the data to choose <span class="math inline">\(m\)</span> within the MCMC algorithm or optimization, we deterministically link the number of neighbors to <span class="math inline">\(\theta_3\)</span> (for our experiments we use <span class="math inline">\(\text{exp}(\theta_3*j) &lt; 0.001\)</span>, where <span class="math inline">\(j\)</span> denotes the neighbor number). This coincides to the amount of variation expected to be learnable from the data. By allowing <span class="math inline">\(m\)</span> to change within the MCMC, an incorrect <span class="math inline">\(m\)</span> will negatively influence the integrated likelihood so the data can reject it.</p>

<p>This section is based on Section 2.2.4 of .</p>
<p>First consider an autoregressive model, then move all elements to the same side.</p>
<p><span class="math display">\[
\begin{aligned}
{\mathbf{y}}_i &amp;= \sum_{j \in g_i} \phi_{ij} z_j + \epsilon_i \\
{\mathbf{y}}_i - \sum_{j \in g_i} \phi_{ij} z_j &amp;= \epsilon_i
\end{aligned}
\]</span></p>
<p>Now, it can be written in matrix form as <span class="math inline">\(\epsilon = T {\mathbf{X}}\)</span>, where</p>
<p><span class="math display">\[
T = \begin{pmatrix}
1 \\
-\phi_{21} &amp; 1 \\
-\phi_{31} &amp; -\phi_{32} &amp; 1 \\
\dots &amp; &amp; &amp; \dots \\
-\phi_{n1} &amp; -\phi_{n2} &amp; \cdots &amp; -\phi_{n n-1} &amp; 1
\end{pmatrix}
\]</span></p>
<p>However, for notational simplicity, we absolve the negative sign into the coefficient matrix <span class="math inline">\({\mathbf{X}}\)</span> Now, to see that it is indeed the valid covariance function:</p>
<p><span class="math display">\[
\begin{aligned}
cov(\epsilon) = D^2 &amp;= cov(TY) = T \Sigma T' \\
\Sigma &amp;= T^{-1} D^2 T'^{-1} \\
\Sigma^{-1} &amp;= T' D^{-2} T
\end{aligned}
\]</span></p>
<p> </p>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by Brian Kidd.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.4.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
